"""
This package provides tools for running qsub jobs on PBS Pro in particular environments.
This avoids boilerplate code for activating environments and switching directories, etc.
In simple cases, the need to create a jobscript can be eliminated entirely.
"""
import sys
import shlex
import time
import logging
import json
import threading
import subprocess
import click
import tailer


class OutputCoordinator:
    """Coordinates between spinner and output threads to prevent display conflicts."""

    def __init__(self):
        self.output_started = threading.Event()
        self.spinner_cleared = threading.Event()

    def signal_output_started(self):
        """Called by tail threads when they start producing output."""
        self.output_started.set()

    def wait_for_output_started(self, timeout=None):
        """Called by spinner to wait for output to start."""
        return self.output_started.wait(timeout)

    def signal_spinner_cleared(self):
        """Called by spinner when it has cleared itself."""
        self.spinner_cleared.set()

    def wait_for_spinner_cleared(self, timeout=None):
        """Called by tail threads to wait for spinner to clear."""
        return self.spinner_cleared.wait(timeout)


def print_status(message, final=False):
    """Print a status message that overwrites the previous one"""
    if final:
        # Final message - print normally and move to next line
        print(f"\r{message}" + " " * 20)
    else:
        # Temporary message - overwrite without newline
        print(f"\r{message}", end="", flush=True)


class JobSpinner:  # pylint: disable=too-many-instance-attributes
    """Context manager for displaying a spinner during job operations."""

    def __init__(self, message="", quiet=False, show_message=True, coordinator=None):
        self.message = message
        self.quiet = quiet
        self.show_message = show_message
        self.coordinator = coordinator
        self.spinner_chars = "‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è"
        self.spinning = False
        self.thread = None
        self.original_line_len = 0

    def _spin(self):
        """Run the spinner animation"""
        i = 0
        while self.spinning:
            # Check if output has started and we need to clear
            if self.coordinator and self.coordinator.wait_for_output_started(timeout=0):
                self._clear_spinner()
                self.coordinator.signal_spinner_cleared()
                break

            char = self.spinner_chars[i % len(self.spinner_chars)]
            if self.show_message:
                line = f"{self.message} {char}"
                print(f"\r{line}", end="", flush=True)
                self.original_line_len = len(line)
            else:
                print(f" {char}", end="", flush=True)
            time.sleep(0.1)
            i += 1

    def _clear_spinner(self):
        """Clear the spinner line"""
        clear_line = " " * (self.original_line_len + 5)
        print(f"\r{clear_line}\r", end="", flush=True)

    def __enter__(self):
        if not self.quiet:
            self.spinning = True
            self.thread = threading.Thread(target=self._spin)
            self.thread.daemon = True
            self.thread.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self.quiet and self.spinning:
            self.spinning = False
            if self.thread:
                self.thread.join(timeout=0.2)
            # Clear the spinner line completely if not already cleared
            if self.coordinator and not self.coordinator.spinner_cleared.is_set():
                self._clear_spinner()
                self.coordinator.signal_spinner_cleared()


def qsub(cmd, quiet=False):
    """
    Submits a job via qsub based on the given command (cmd)

    Args:
        cmd (chr): qsub command, including all of the options generated by qt
        quiet (bool): whether to suppress spinner output

    Returns:
        The job id for the submitted job
    """
    # Make sure that this is a qsub cmd -- don't pass on random commands!
    if not cmd[:4] == "qsub":
        click.echo("Expected qsub comment. Exiting")
        sys.exit(1)

    with JobSpinner(show_message=False, quiet=quiet):
        # pylint: disable=W1510
        result = subprocess.run(shlex.split(cmd),
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                text=True)
        if result.returncode != 0:
            logging.debug("Job submission failed")
            click.echo(result.stderr)
            sys.exit(result.returncode)
        else:
            logging.debug("Job submitted successfully")
            return result.stdout.rstrip('\n')

def qdel(job_id, quiet=False):
    """
    Deletes a PBS job using qdel command

    Args:
        job_id (str): The PBS job id to delete
        quiet (bool): whether to suppress output

    Returns:
        bool: True if deletion was successful, False otherwise
    """
    logging.debug("Deleting job %s", job_id)

    try:
        # pylint: disable=W1510
        result = subprocess.run(['qdel', job_id],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                text=True)
        if result.returncode == 0:
            if not quiet:
                click.echo(f"üóëÔ∏è  Job {job_id} deleted successfully")
            logging.info("Job %s deleted successfully", job_id)
            return True

        if not quiet:
            click.echo(f"Failed to delete job {job_id}: {result.stderr.strip()}")
        logging.warning("Failed to delete job %s: %s", job_id, result.stderr.strip())
        return False
    except Exception as e:  # pylint: disable=broad-except
        if not quiet:
            click.echo(f"Error deleting job {job_id}: {e}")
        logging.error("Error deleting job %s: %s", job_id, e)
        return False

def job_status(job_id):
    """
    Parses 'qstat' to determine the status of the job with the given id

    Returns:
        Q: queued
        R: running
        E: ending
        F: finished
        H: held (not enough quota)
    """
    logging.debug("Job id: %s", job_id)
    # pylint: disable=W1510
    qstat_result = subprocess.run(['qstat', '-x', '-f', '-F', 'json', job_id],
                                  stdout=subprocess.PIPE, text=True)
    # Check if qstat failed
    if qstat_result.returncode != 0:
        logging.debug("qstat failed for job %s", job_id)
        click.echo(qstat_result.stderr)
        click.echo(qstat_result.args)
        sys.exit(qstat_result.returncode)
    # Otherwise, extract and return job status from qstat results
    qstat_info = json.loads(qstat_result.stdout)
    logging.debug("Job status: %s", qstat_info)
    status=qstat_info['Jobs'][job_id]['job_state']
    logging.debug("Job status %s", status)
    return status

def monitor_qstat(job_id, quiet=False, coordinator=None, success_msg=None):
    """
    Monitors for job completion by checking job status periodically
    """
    logging.debug("Waiting 60 seconds before checking job status")

    # Use the provided success message or create a default one
    if success_msg:
        spinner_msg = f"{success_msg} - Waiting for job to start..."
    else:
        spinner_msg = f"Job ID: {job_id} - Waiting for job to start..."

    with JobSpinner(spinner_msg, show_message=True, quiet=quiet, coordinator=coordinator):
        time.sleep(60)

    logging.debug("Starting job monitoring")

    while True:
        status = job_status(job_id)
        if status in ['F', 'H']:  # Check for job completion
            logging.info("Job %s completed", job_id)
            # Job completed - exit silently (user can check qstat if needed)
            sys.exit(0)
        logging.debug("Job %s still running", job_id)
        # Sleep without showing any status messages
        time.sleep(30)  # Poll every 30 seconds

def tail(log_file, destination, coordinator=None):
    """
    Tails the given log_file until either EOF or job completion.
    Output is directed to the specified destination (STDOUT or STDERR)
    """
    if not destination in ['STDOUT', 'STDERR']:
        click.echo("Unknown destination for redirection")
        sys.exit(2)
    is_err = destination == 'STDERR'
    output_started = False

    with open(log_file, 'r', encoding='utf-8') as f:
        for line in tailer.follow(f):
            # Signal that output has started on first line
            if not output_started and coordinator:
                coordinator.signal_output_started()
                # Clear the waiting line completely - use longer clear for safety
                print("\r" + " " * 120 + "\r", end="", flush=True)
                coordinator.signal_spinner_cleared()
                output_started = True

            print(line.rstrip(), file=sys.stderr if is_err else sys.stdout)

def monitor_and_tail(job_id, out_file, err_file, quiet=False, success_msg=None):
    """
    Monitors the job status using qstat and tails the output (STDOUT and STDERR) logs
    until the job is finished. Stops both tailing and monitoring upon job completion.

    Args:
        job_id: The PBS job id to monitor.
        out_file: Path to the STDOUT log file to tail.
        err_file: Path to the STDERR log file to tail.
        quiet: Whether to suppress spinner output.
        success_msg: The success message to display with spinner.
    """
    # Create coordinator for thread synchronization
    coordinator = OutputCoordinator()

    # Create threads for job monitoring and log tailing
    qstat_thread = threading.Thread(target=monitor_qstat,
                                     args=(job_id, quiet, coordinator, success_msg))
    out_thread = threading.Thread(target=tail,
                                  args=(out_file, "STDOUT", coordinator),
                                  daemon=True)
    err_thread = threading.Thread(target=tail,
                                  args=(err_file, "STDERR", coordinator),
                                  daemon=True)

    # Start all threads
    qstat_thread.start()
    out_thread.start()
    err_thread.start()
    # Wait for job monitoring to complete
    qstat_thread.join()
