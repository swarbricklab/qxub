"""
This package provides tools for running qsub jobs on PBS Pro in particular environments.
This avoids boilerplate code for activating environments and switching directories, etc.
In simple cases, the need to create a jobscript can be eliminated entirely.
"""

import sys
import shlex
import signal
import time
import logging
import threading
import subprocess
import click
import tailer
from .resource_parser import (
    size_to_bytes,
    time_to_seconds,
    parse_exec_host,
    parse_exec_vnode,
    parse_timestamp,
    calculate_efficiency,
    bytes_to_human,
)


class OutputCoordinator:
    """Coordinates between spinner, output threads, and monitoring threads."""

    def __init__(self):
        self.output_started = threading.Event()
        self.spinner_cleared = threading.Event()
        self.job_completed = threading.Event()
        self.shutdown_requested = threading.Event()
        self.eof_detected = threading.Event()
        self.job_exit_status = None  # Store the job's exit status

    def signal_output_started(self):
        """Called by tail threads when they start producing output."""
        self.output_started.set()

    def wait_for_output_started(self, timeout=None):
        """Called by spinner to wait for output to start."""
        return self.output_started.wait(timeout)

    def signal_spinner_cleared(self):
        """Called by spinner when it has cleared itself."""
        self.spinner_cleared.set()

    def signal_job_completed(self):
        """Called by monitor when job completes."""
        self.job_completed.set()

    def signal_shutdown(self):
        """Called when shutdown is requested (e.g., Ctrl-C)."""
        self.shutdown_requested.set()

    def signal_eof(self):
        """Called by tail threads when they reach EOF."""
        self.eof_detected.set()

    def should_shutdown(self):
        """Check if threads should shutdown."""
        return (
            self.shutdown_requested.is_set()
            or self.job_completed.is_set()
            or self.eof_detected.is_set()
        )

    def wait_for_spinner_cleared(self, timeout=None):
        """Called by tail threads to wait for spinner to clear."""
        return self.spinner_cleared.wait(timeout)


def print_status(message, final=False):
    """Print a status message that overwrites the previous one"""
    if final:
        # Final message - print normally and move to next line
        print(f"\r{message}" + " " * 20)
    else:
        # Temporary message - overwrite without newline
        print(f"\r{message}", end="", flush=True)


class JobSpinner:  # pylint: disable=too-many-instance-attributes
    """Context manager for displaying a spinner during job operations."""

    def __init__(self, message="", quiet=False, show_message=True, coordinator=None):
        self.message = message
        self.quiet = quiet
        self.show_message = show_message
        self.coordinator = coordinator
        self.spinner_chars = "‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è"
        self.spinning = False
        self.thread = None
        self.original_line_len = 0

    def _spin(self):
        """Run the spinner animation"""
        i = 0
        while self.spinning:
            # Check if output has started and we need to clear
            if self.coordinator and self.coordinator.wait_for_output_started(timeout=0):
                self._clear_spinner()
                self.coordinator.signal_spinner_cleared()
                break

            char = self.spinner_chars[i % len(self.spinner_chars)]
            if self.show_message:
                line = f"{self.message} {char}"
                print(f"\r{line}", end="", flush=True)
                self.original_line_len = len(line)
            else:
                print(f" {char}", end="", flush=True)
            time.sleep(0.1)
            i += 1

    def _clear_spinner(self):
        """Clear the spinner line"""
        clear_line = " " * (self.original_line_len + 5)
        print(f"\r{clear_line}\r", end="", flush=True)

    def __enter__(self):
        if not self.quiet:
            self.spinning = True
            self.thread = threading.Thread(target=self._spin)
            self.thread.daemon = True
            self.thread.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self.quiet and self.spinning:
            self.spinning = False
            if self.thread:
                self.thread.join(timeout=0.2)
            # Clear the spinner line completely if not already cleared
            if self.coordinator and not self.coordinator.spinner_cleared.is_set():
                self._clear_spinner()
                self.coordinator.signal_spinner_cleared()


def qsub(cmd, quiet=False):
    """
    Submits a job via qsub based on the given command (cmd)

    Args:
        cmd (chr): qsub command, including all of the options generated by qt
        quiet (bool): whether to suppress spinner output

    Returns:
        The job id for the submitted job
    """
    # Make sure that this is a qsub cmd -- don't pass on random commands!
    if not cmd[:4] == "qsub":
        click.echo("Expected qsub comment. Exiting")
        sys.exit(1)

    with JobSpinner(show_message=False, quiet=quiet):
        # pylint: disable=W1510
        result = subprocess.run(
            shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        if result.returncode != 0:
            logging.debug("Job submission failed")
            click.echo(result.stderr)

            # Map PBS validation errors to exit code 2 for consistency
            if result.returncode == 166:  # PBS validation error
                sys.exit(2)
            else:
                sys.exit(result.returncode)
        else:
            logging.debug("Job submitted successfully")
            return result.stdout.rstrip("\n")


def qdel(job_id, quiet=False):
    """
    Deletes a PBS job using qdel command

    Args:
        job_id (str): The PBS job id to delete
        quiet (bool): whether to suppress output

    Returns:
        bool: True if deletion was successful, False otherwise
    """
    logging.debug("Deleting job %s", job_id)

    try:
        # pylint: disable=W1510
        result = subprocess.run(
            ["qdel", job_id], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        if result.returncode == 0:
            if not quiet:
                click.echo(f"üóëÔ∏è  Job {job_id} deleted successfully")
            logging.info("Job %s deleted successfully", job_id)
            return True

        if not quiet:
            click.echo(f"Failed to delete job {job_id}: {result.stderr.strip()}")
        logging.warning("Failed to delete job %s: %s", job_id, result.stderr.strip())
        return False
    except Exception as e:  # pylint: disable=broad-except
        if not quiet:
            click.echo(f"Error deleting job {job_id}: {e}")
        logging.error("Error deleting job %s: %s", job_id, e)
        return False


def job_status(job_id):
    """
    Check the current status of a job.

    Returns:
        H: held (not enough quota)
    """
    logging.debug("Job id: %s", job_id)

    # Use DSV format with ASCII Unit Separator for robust parsing
    delimiter = "\x1f"  # ASCII Unit Separator - designed for field separation
    qstat_result = subprocess.run(
        ["qstat", "-fx", "-F", "dsv", "-D", delimiter, job_id],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        check=False,
    )

    # Check if qstat failed
    if qstat_result.returncode != 0:
        logging.debug("qstat failed for job %s", job_id)
        click.echo(qstat_result.stderr)
        click.echo(qstat_result.args)
        sys.exit(qstat_result.returncode)

    # Parse DSV output to find job_state
    output = qstat_result.stdout.strip()
    if not output:
        logging.error("Empty qstat output for job %s", job_id)
        return "C"  # Assume completed if no output


def wait_for_job_exit_status(job_id, initial_wait=5, poll_interval=5, max_attempts=12):
    """
    Wait for PBS to clean up the job and get its exit status.

    Args:
        job_id: The PBS job ID
        initial_wait: Initial wait time for PBS cleanup (seconds)
        poll_interval: Interval between polling attempts (seconds)
        max_attempts: Maximum number of polling attempts

    Returns:
        int: The job's exit status, or 1 if unavailable
    """
    logging.debug("Waiting %d seconds for PBS cleanup", initial_wait)
    time.sleep(initial_wait)

    for attempt in range(max_attempts):
        exit_status = job_exit_status(job_id)
        if exit_status is not None:
            logging.info("Job %s exit status: %d", job_id, exit_status)
            return exit_status

        if attempt < max_attempts - 1:  # Don't sleep on the last attempt
            logging.debug(
                "Exit status not available yet, waiting %d seconds (attempt %d/%d)",
                poll_interval,
                attempt + 1,
                max_attempts,
            )
            time.sleep(poll_interval)

    logging.warning("Could not get exit status for job %s after %d attempts", job_id, max_attempts)
    return 1  # Return failure exit code if we can't determine the real status


def job_exit_status(job_id):
    """
    Get the exit status of a completed job.

    Returns:
        int: The job's exit status, or None if not available
    """
    job_data = get_job_resource_data(job_id)
    if job_data and "exit_status" in job_data:
        return job_data["exit_status"]
    return None


def get_job_resource_data(job_id):
    """
    Get comprehensive resource data for a completed job from qstat -fx.

    Args:
        job_id: The PBS job ID

    Returns:
        dict: Complete job resource information, or None if not available

    The returned dictionary contains:
        - exit_status: Job exit code
        - resources_requested: Dict of requested resources
        - resources_used: Dict of actually used resources
        - execution: Dict of execution environment info
        - timing: Dict of job timing information
        - metadata: Dict of job metadata (name, owner, etc.)
    """
    logging.debug("Getting resource data for job %s", job_id)

    # Use standard qstat -fx format for full resource information
    qstat_result = subprocess.run(
        ["qstat", "-fx", job_id],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        check=False,
    )

    if qstat_result.returncode != 0:
        logging.debug("qstat failed for job %s: %s", job_id, qstat_result.stderr)
        return None

    output = qstat_result.stdout.strip()
    if not output:
        logging.debug("Empty qstat output for job %s", job_id)
        return None

    return parse_qstat_fx_output(output)


def parse_qstat_fx_output(qstat_output):
    """
    Parse qstat -fx output to extract comprehensive job resource information.

    Args:
        qstat_output: Raw output from qstat -fx command

    Returns:
        dict: Parsed job resource data
    """
    if not qstat_output:
        return None

    data = {
        "exit_status": None,
        "resources_requested": {},
        "resources_used": {},
        "execution": {},
        "timing": {},
        "metadata": {},
    }

    # Parse line by line
    for line in qstat_output.split("\n"):
        line = line.strip()
        if not line or not "=" in line:
            continue

        # Split on first '=' to handle values with '=' in them
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip()

        # Parse different categories of information
        if key == "Exit_status":
            try:
                data["exit_status"] = int(value)
            except ValueError:
                logging.warning("Could not parse exit status: %s", value)

        elif key.startswith("Resource_List."):
            # Requested resources
            resource_key = key[14:]  # Remove 'Resource_List.' prefix
            data["resources_requested"][resource_key] = value

        elif key.startswith("resources_used."):
            # Used resources
            resource_key = key[15:]  # Remove 'resources_used.' prefix
            data["resources_used"][resource_key] = value

        elif key in ["exec_host", "exec_vnode", "queue", "session_id", "project", "group_list"]:
            # Execution environment
            data["execution"][key] = value

        elif key in ["qtime", "stime", "etime", "mtime", "ctime", "obittime"]:
            # Timing information
            data["timing"][key] = value

        elif key in ["Job_Id", "Job_Name", "Job_Owner", "job_state"]:
            # Job metadata
            data["metadata"][key] = value

    # Post-process resource data for easier consumption
    data = _enhance_resource_data(data)

    return data


def _enhance_resource_data(data):
    """
    Enhance parsed resource data with calculated fields and conversions.

    Args:
        data: Raw parsed data from parse_qstat_fx_output

    Returns:
        dict: Enhanced data with calculated efficiency metrics
    """
    if not data:
        return data

    # Convert resource values to standardized formats
    requested = data["resources_requested"]
    used = data["resources_used"]

    # Parse timing information
    timing = data["timing"]
    for time_key in list(timing.keys()):  # Convert to list to avoid iteration issues
        if timing[time_key]:
            parsed_time = parse_timestamp(timing[time_key])
            if parsed_time:
                timing[f"{time_key}_parsed"] = parsed_time.isoformat()

    # Calculate derived timing metrics
    if "qtime_parsed" in timing and "stime_parsed" in timing:
        try:
            from datetime import datetime

            qtime = datetime.fromisoformat(timing["qtime_parsed"])
            stime = datetime.fromisoformat(timing["stime_parsed"])
            timing["queue_wait_seconds"] = int((stime - qtime).total_seconds())
        except Exception:
            pass

    if "stime_parsed" in timing and "mtime_parsed" in timing:
        try:
            from datetime import datetime

            stime = datetime.fromisoformat(timing["stime_parsed"])
            mtime = datetime.fromisoformat(timing["mtime_parsed"])
            timing["execution_seconds"] = int((mtime - stime).total_seconds())
        except Exception:
            pass

    # Parse execution environment
    execution = data["execution"]
    if "exec_host" in execution:
        execution["exec_host_parsed"] = parse_exec_host(execution["exec_host"])
    if "exec_vnode" in execution:
        execution["exec_vnode_parsed"] = parse_exec_vnode(execution["exec_vnode"])

    # Calculate resource efficiency metrics
    efficiency = {}

    # Memory efficiency
    if "mem" in requested and "mem" in used:
        try:
            mem_requested_bytes = size_to_bytes(requested["mem"])
            mem_used_bytes = size_to_bytes(used["mem"])
            efficiency["memory_efficiency"] = calculate_efficiency(
                mem_used_bytes, mem_requested_bytes
            )
            efficiency["memory_requested_human"] = bytes_to_human(mem_requested_bytes)
            efficiency["memory_used_human"] = bytes_to_human(mem_used_bytes)
        except Exception as e:
            logging.warning("Could not calculate memory efficiency: %s", e)

    # Time efficiency
    if "walltime" in requested and "walltime" in used:
        try:
            time_requested_seconds = time_to_seconds(requested["walltime"])
            time_used_seconds = time_to_seconds(used["walltime"])
            efficiency["time_efficiency"] = calculate_efficiency(
                time_used_seconds, time_requested_seconds
            )
        except Exception as e:
            logging.warning("Could not calculate time efficiency: %s", e)

    # CPU efficiency (from cpupercent if available)
    if "cpupercent" in used:
        try:
            efficiency["cpu_efficiency"] = float(used["cpupercent"])
        except Exception:
            pass

    # Jobfs efficiency
    if "jobfs" in requested and "jobfs" in used:
        try:
            jobfs_requested_bytes = size_to_bytes(requested["jobfs"])
            jobfs_used_bytes = size_to_bytes(used["jobfs"])
            efficiency["jobfs_efficiency"] = calculate_efficiency(
                jobfs_used_bytes, jobfs_requested_bytes
            )
            efficiency["jobfs_requested_human"] = bytes_to_human(jobfs_requested_bytes)
            efficiency["jobfs_used_human"] = bytes_to_human(jobfs_used_bytes)
        except Exception as e:
            logging.warning("Could not calculate jobfs efficiency: %s", e)

    data["efficiency"] = efficiency

    return data


def job_status(job_id):
    """
    Check the current status of a completed job_status function.

    Returns:
        string: Job state (R, Q, H, F, C, etc.)
    """
    logging.debug("Getting job status for job %s", job_id)

    # Use DSV format with ASCII Unit Separator for robust parsing
    delimiter = "\x1f"  # ASCII Unit Separator - designed for field separation
    qstat_result = subprocess.run(
        ["qstat", "-fx", "-F", "dsv", "-D", delimiter, job_id],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        check=False,
    )

    # Check if qstat failed
    if qstat_result.returncode != 0:
        logging.debug("qstat failed for job %s", job_id)
        return "C"  # Assume completed if qstat fails

    # Parse DSV output to find job_state
    output = qstat_result.stdout.strip()
    if not output:
        logging.error("Empty qstat output for job %s", job_id)
        return "C"  # Assume completed if no output

    # Split on delimiter and look for job_state field
    fields = output.split(delimiter)
    for field in fields:
        if field.startswith("job_state="):
            status = field.split("=", 1)[1]
            logging.debug("Job status: %s", status)
            return status

    # Fallback: if job_state not found, try simple qstat
    logging.warning("job_state field not found in DSV output, trying simple qstat")
    try:
        fallback_result = subprocess.run(
            ["qstat", job_id],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=False,
        )
        if fallback_result.returncode == 0 and fallback_result.stdout:
            # Parse simple qstat output
            lines = fallback_result.stdout.strip().split("\n")
            if len(lines) >= 2:  # Header + job line
                job_line = lines[-1].split()
                if len(job_line) >= 8:
                    status = job_line[-1]  # Last column is status
                    logging.debug("Job status from simple qstat: %s", status)
                    return status

        # Final fallback - assume completed
        logging.warning("All qstat methods failed, assuming job completed")
        return "C"
    except Exception as e:
        logging.error("Exception in job_status fallback: %s", e)
        return "C"


def monitor_qstat(job_id, quiet=False, coordinator=None, success_msg=None):
    """
    Monitors for job completion by checking job status periodically
    """
    logging.debug("Waiting 60 seconds before checking job status")

    # Use the provided success message or create a default one
    if success_msg:
        spinner_msg = f"{success_msg} - Waiting"
    else:
        spinner_msg = f"Job {job_id[:8]} - Waiting"

    with JobSpinner(spinner_msg, show_message=True, quiet=quiet, coordinator=coordinator):
        time.sleep(60)

    logging.debug("Starting job monitoring")

    while True:
        # Check if we should shutdown early
        if coordinator and coordinator.should_shutdown():
            logging.info("Monitor shutdown requested")
            return

        status = job_status(job_id)
        if status in ["F", "H"]:  # Check for job completion
            logging.info("Job %s completed with status %s", job_id, status)
            if coordinator:
                coordinator.signal_job_completed()

            # Wait for PBS cleanup and get exit status
            logging.info("Waiting for PBS cleanup and getting exit status...")
            exit_status = wait_for_job_exit_status(job_id)
            if coordinator:
                coordinator.job_exit_status = exit_status
            return exit_status

        logging.debug("Job %s still running", job_id)

        # Sleep in smaller intervals so we can respond to shutdown requests
        for _ in range(6):  # 6 * 5 = 30 seconds total
            if coordinator and coordinator.should_shutdown():
                logging.info("Monitor shutdown during sleep")
                return 0  # Return success if interrupted
            time.sleep(5)


def tail(log_file, destination, coordinator=None):
    """
    Tails the given log_file until either EOF or job completion.
    Output is directed to the specified destination (STDOUT or STDERR)
    """
    if not destination in ["STDOUT", "STDERR"]:
        click.echo("Unknown destination for redirection")
        sys.exit(2)
    is_err = destination == "STDERR"
    output_started = False

    try:
        with open(log_file, "r", encoding="utf-8") as f:
            for line in tailer.follow(f):
                # Check if we should shutdown
                if coordinator and coordinator.should_shutdown():
                    logging.debug(f"Tail {destination} shutdown requested")
                    break

                # Signal that output has started on first line
                if not output_started and coordinator:
                    coordinator.signal_output_started()
                    # Clear the waiting line completely - use longer clear for safety
                    print("\r" + " " * 120 + "\r", end="", flush=True)
                    coordinator.signal_spinner_cleared()
                    output_started = True

                print(line.rstrip(), file=sys.stderr if is_err else sys.stdout)

            # If we exit the loop normally, we've reached EOF
            if coordinator:
                coordinator.signal_eof()
                logging.debug(f"Tail {destination} reached EOF")

    except Exception as e:
        logging.error(f"Error in tail {destination}: {e}")
    finally:
        logging.debug(f"Tail {destination} thread exiting")


def monitor_and_tail(job_id, out_file, err_file, quiet=False, success_msg=None):
    """
    Monitors the job status using qstat and tails the output (STDOUT and STDERR) logs
    until the job is finished. Stops both tailing and monitoring upon job completion.

    Args:
        job_id: The PBS job id to monitor.
        out_file: Path to the STDOUT log file to tail.
        err_file: Path to the STDERR log file to tail.
        quiet: Whether to suppress spinner output.
        success_msg: The success message to display with spinner.

    Returns:
        int: The job's exit status
    """
    # Create coordinator for thread synchronization
    coordinator = OutputCoordinator()

    # Create threads for job monitoring and log tailing
    qstat_thread = threading.Thread(
        target=monitor_qstat, args=(job_id, quiet, coordinator, success_msg)
    )
    out_thread = threading.Thread(target=tail, args=(out_file, "STDOUT", coordinator), daemon=True)
    err_thread = threading.Thread(target=tail, args=(err_file, "STDERR", coordinator), daemon=True)

    # Set up signal handler for Ctrl-C
    def signal_handler(signum, frame):
        logging.info("Interrupt received, shutting down threads...")
        coordinator.signal_shutdown()

    original_handler = signal.signal(signal.SIGINT, signal_handler)

    try:
        # Start all threads
        qstat_thread.start()
        out_thread.start()
        err_thread.start()

        # Wait for job monitoring to complete or shutdown signal
        qstat_thread.join()

        # Signal shutdown to remaining threads
        coordinator.signal_shutdown()

        # Give threads a moment to cleanup gracefully
        time.sleep(0.5)

        logging.info("Job monitoring completed")

        # Return the job's exit status
        return coordinator.job_exit_status if coordinator.job_exit_status is not None else 0

    except KeyboardInterrupt:
        logging.info("KeyboardInterrupt received, shutting down...")
        coordinator.signal_shutdown()
        qstat_thread.join(timeout=2)  # Give monitor thread time to cleanup
        return 130  # Standard exit code for SIGINT

    finally:
        # Restore original signal handler
        signal.signal(signal.SIGINT, original_handler)
